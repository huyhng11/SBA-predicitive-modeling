# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1udnPt0mfu7hlfQBEe8B4SclP9eQ7IoVN

1. **Load and Run correlation matrix**
"""


import pandas as pd
import matplotlib.pyplot as plt

# 1) Load
df = pd.read_csv('SBAnational.csv')
df.head()
print(df.dtypes)

# Clean currency columns
currency_cols = ['DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv', 'ChgOffPrinGr']
for col in currency_cols:
    df[col] = pd.to_numeric(df[col].astype(str).str.replace('[\$,]', '', regex=True), errors='coerce')

# Parse datetime columns
df['ApprovalDate'] = pd.to_datetime(df['ApprovalDate'], errors='coerce')
df['DisbursementDate'] = pd.to_datetime(df['DisbursementDate'], errors='coerce')
df['ChgOffDate'] = pd.to_datetime(df['ChgOffDate'], errors='coerce')

# Target variable
df['MIS_Status_Binary'] = df['MIS_Status'].map({'P I F': 0, 'CHGOFF': 1})
df['LowDoc_Binary'] = df['LowDoc'].map({'Y': 1, 'N': 0})
df['RevLineCr_Binary'] = df['RevLineCr'].map({'Y': 1, 'N': 0})
df['UrbanRural_Numeric'] = pd.to_numeric(df['UrbanRural'], errors='coerce')

numeric_features = [
    'Term', 'NoEmp', 'CreateJob', 'RetainedJob',
    'FranchiseCode', 'DisbursementGross', 'BalanceGross',
    'ChgOffPrinGr', 'GrAppv', 'SBA_Appv',
    'LowDoc_Binary', 'RevLineCr_Binary', 'UrbanRural_Numeric',
    'MIS_Status_Binary'
]

df_corr = df[numeric_features].dropna()

# Compute correlation matrix
correlation_matrix = df_corr.corr()
print(correlation_matrix)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Selected Numeric Features", fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

import numpy as np

# Mask to get only upper triangle, to avoid duplicate pairs
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)

# Extract correlation values from upper triangle
corr_pairs = correlation_matrix.where(mask).stack()

# Filter pairs with correlation > 0.4
strong_corr = corr_pairs[corr_pairs > 0.4]

print("Correlation pairs with correlation > 0.4:")
print(strong_corr)

"""2. **New vs Established Business**"""

df['NewExist'].value_counts()

df_filtered = df[df['NewExist'].isin([1, 2])]

default_rates = df_filtered.groupby('NewExist')['MIS_Status_Binary'].mean()

business_type_map = {1: 'New', 2: 'Established'}
default_rates.index = default_rates.index.map(business_type_map)

print("Default rate by business status:")
print(default_rates)

# Plot default rates
default_rates.plot(kind='bar', color=['orange', 'skyblue'])
plt.ylabel('Default Rate')
plt.title('Default Rate: New vs Established Businesses')
plt.xticks(rotation=0)
plt.show()

"""The default rate of 2 business models does not differ much, suggesting that business longevity would not be a good indicator for default rate

3. **Loans Backed by Real Estate**
"""

df['RealEstate']=df['Term'].apply(lambda x: 1 if x >= 240 else 0)
df['RealEstate'].value_counts()

grouped = df.groupby('RealEstate')['MIS_Status_Binary']
default_rate = grouped.mean()
pif_rate     = 1 - default_rate

rates = pd.DataFrame({
    'Default Rate': default_rate,
    'Pay-in-Full Rate': pif_rate
}).rename(index={0: 'Non Real Estate Backed (<240m)', 1: 'Real Estate Backed (≥240m)'})

print(rates)

rates.plot(kind='bar', figsize=(8,5))
plt.ylim(0,1)
plt.ylabel('Rate')
plt.title('Default vs. Pay-in-Full Rates\nby Real-Estate Backing')
plt.xticks(rotation=0)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

"""Loans backed by real estate exhibit a dramatically lower default rate (1.6% vs. 20.8%) and a correspondingly higher pay-in-full rate (98.4% vs. 79.2%). This aligns perfectly with the collateral hypothesis: long-term, real-estate-secured loans carry far less risk of charge-off because the underlying property value typically covers outstanding principal

4. **Economic Recession**
"""

recession_start = pd.to_datetime('2007-12-01')
recession_end   = pd.to_datetime('2009-06-30')

# Make sure ApprovalDate is datetime and drop rows missing it
df = df[df['ApprovalDate'].notna()]

# Create Recession dummy:
df['Recession'] = df['ApprovalDate'].between(recession_start, recession_end).astype(int)
print(df['Recession'].value_counts())

# Compute default and pay-in-full rates by Recession flag
grouped = df.groupby('Recession')['MIS_Status_Binary']
default_rate = grouped.mean()            # mean of 1/0 default flags
pif_rate     = 1 - default_rate

rates = pd.DataFrame({
    'Default Rate': default_rate,
    'Pay-in-Full Rate': pif_rate
}).rename(index={0: 'Outside Recession', 1: 'During Recession'})

print(rates)

ax = rates.plot(
    kind='bar',
    stacked=True,
    figsize=(8, 5)
)

ax.set_ylabel('Proportion')
ax.set_ylim(0, 1)
ax.set_title('Loan Outcomes by Recession Period (Stacked)')
ax.legend(loc='upper right')

for container in ax.containers:
    ax.bar_label(container, fmt='%.3f', label_type='center')

plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

"""- The patter is clear, during recession companies have higher default rate and lower PIF. Therefore, it is suggested that loan performance is highly sensitive to macroeconomic conditions. Underwriting standards that look sound in growth periods can yield sharply higher losses in downturns
- Regulators or risk managers should stress‐test portfolios against recession-like scenarios—SBA portfolios could see defaults more than double under severe downturns

5. **SBA’s Guaranteed Portion of Approved Loan**
"""

df_sba = df[['SBA_Appv', 'MIS_Status']].dropna()
df_sba = df[df['MIS_Status'].isin(['P I F', 'CHGOFF'])]

# Visualize using a boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_sba, x='MIS_Status', y='SBA_Appv')
plt.title('Distribution of SBA Guaranteed Amount by Loan Status')
plt.xlabel('MIS_Status (P I F = Paid in Full, CHGOFF = Charged Off)')
plt.ylabel('SBA Guaranteed Amount ($)')
plt.grid(True)
plt.show()

"""- The median SBA guaranteed amount is higher for PIF loans than for CHGOFF loans --> Suggests that loans with higher guaranteed amounts are more likely to be paid back in full

- The IQR has the box height for PIF is larger, showing more variability among successfully repaid loans --> PIF have more outliers while Charge Off have less, indicating less success rate when the SBA's loan proportion covered is less

6. **Default Rate by Industry, State**
"""

state_stats = df.groupby('State').agg(
    loan_count      = ('MIS_Status_Binary','size'),
    default_rate    = ('MIS_Status_Binary','mean')
).reset_index()

top10 = state_stats.sort_values('default_rate', ascending=False).head(10)

print("Top 10 States by SBA Default Rate :")
print(top10[['State','loan_count','default_rate']])

plt.bar(top10['State'], top10['default_rate'])
plt.ylim(0, top10['default_rate'].max() * 1.1)
plt.ylabel('Default Rate')
plt.xlabel('State')
plt.title('Top 10 States by SBA Loan Default Rate\n')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""FL has by far the highest default rate at 27.4%, nearly 4 points above the next state. This suggests that, during your sample period, SBA‐backed loans in Florida carried substantially more risk than in other large states"""

df.columns

df['NAICS2'] = df['NAICS'].astype(int).astype(str).str[:2]

# Compute counts and default rates by sector
industry_stats = (
    df
      .groupby('NAICS2')['MIS_Status_Binary']
      .agg(loan_count='size', default_rate='mean')
      .reset_index()
)


# Sort and take top 10 riskiest sectors
top10_ind = industry_stats.sort_values('default_rate', ascending=False).head(10)

print("Top 10 NAICS 2-digit Sectors by Default Rate")
print(top10_ind)

# Plot
fig, ax = plt.subplots(figsize=(10,6))
bars = ax.bar(top10_ind['NAICS2'], top10_ind['default_rate'] * 100, color='teal')

# Label each bar with percent
ax.bar_label(bars, labels=[f"{x:2f}%" for x in top10_ind['default_rate'] * 100], padding=4)

ax.set_ylim(0, top10_ind['default_rate'].max() * 110)
ax.set_ylabel('Default Rate (%)')
ax.set_xlabel('NAICS 2-Digit Sector')
ax.set_title(f'Top 10 SBA Default Rates by Industry Sector\n(NAICS 2-Digits)')
plt.tight_layout()
plt.show()

"""| NAICS 2-Digit | Sector                                      | Loan Count | Default Rate |
| ------------: | ------------------------------------------- | ---------: | -----------: |
|            53 | Real Estate & Rental & Leasing              |     13,632 |        28.7% |
|            52 | Finance & Insurance                         |      9,496 |        28.4% |
|            48 | Transportation & Warehousing                |     20,310 |        26.9% |
|            51 | Information                                 |     11,379 |        24.8% |
|            61 | Educational Services                        |      6,425 |        24.2% |
|            56 | Administrative & Support & Waste Management |     32,685 |        23.6% |
|            45 | Retail Trade                                |     42,514 |        23.4% |
|            23 | Construction                                |     66,646 |        23.3% |
|            49 | Transit & Ground Passenger Transportation   |      2,221 |        23.0% |
|            44 | Retail Trade (second sub-category)          |     84,737 |        22.4% |

"""

df.columns

"""**TRAIN THE MODEL**

# Feature selection and define X and y**
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, make_scorer

# Recompute engineered feature
df['GuaranteeRatio'] = df['SBA_Appv'] / df['GrAppv']

# Check for NoEmployee
df['NoEmp'] = pd.to_numeric(df['NoEmp'], errors='coerce')

# 3. Drop rows where NoEmp or MIS_Status_Binary is missing
df_clean = df[['NoEmp', 'MIS_Status_Binary']].dropna()

# 4. Compute Pearson correlation coefficient between NoEmp and default flag
corr_coef = df_clean['NoEmp'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (NoEmp vs. Default): {corr_coef:.4f}")

# 5. Calculate default rate by each distinct employee count
#    (If there are too many distinct employee counts, consider binning instead.)
grouped = df_clean.groupby('NoEmp')['MIS_Status_Binary'].agg(
    loan_count='size',
    default_rate='mean'
).reset_index()

plt.figure(figsize=(8, 5))
plt.scatter(grouped['NoEmp'], grouped['default_rate'], s=20, alpha=0.6)
plt.xlabel('Number of Employees (NoEmp)')
plt.ylabel('Default Rate')
plt.title('Default Rate by Exact Number of Employees')
plt.grid(True)
plt.tight_layout()
plt.show()

# Check for 
df['CreateJob'] = pd.to_numeric(df['CreateJob'], errors='coerce')

# 3. Drop rows where CreateJob or MIS_Status_Binary is missing
df_clean = df[['CreateJob', 'MIS_Status_Binary']].dropna()

# 4. Compute Pearson correlation coefficient between CreateJob and default flag
corr_coef = df_clean['CreateJob'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (CreateJob vs. Default): {corr_coef:.4f}")

# 5. Group by each distinct CreateJob value to calculate default rate per job created
grouped = (
    df_clean
    .groupby('CreateJob')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

print("\nDefault rate by exact CreateJob count:")
print(grouped.head(10))  # print first 10 rows for inspection

# 6. Plot default rate vs. exact CreateJob count (scatter)
plt.figure(figsize=(8, 5))
plt.scatter(grouped['CreateJob'], grouped['default_rate'], s=20, alpha=0.6)
plt.xlabel('Number of Jobs Created (CreateJob)')
plt.ylabel('Default Rate')
plt.title('Default Rate by Exact CreateJob Count')
plt.grid(True)
plt.tight_layout()
plt.show()

bins = [-1, 0, 5, 10, 20, df_clean['CreateJob'].max()+1]
labels = ['0', '1–5', '6–10', '11–20', '21+']
df_clean['CreateJob_Bin'] = pd.cut(df_clean['CreateJob'], bins=bins, labels=labels, right=False)

grouped = (
    df_clean
    .groupby('CreateJob_Bin')['MIS_Status_Binary']
    .mean()
    .reset_index()
)
print(grouped)
## No significant relationship

# Retained Job
df['RetainedJob'] = pd.to_numeric(df['RetainedJob'], errors='coerce')

# 3. Drop rows where RetainedJob or MIS_Status_Binary is missing
df_clean = df[['RetainedJob', 'MIS_Status_Binary']].dropna()

# 4. Compute Pearson correlation coefficient between RetainedJob and default flag
corr_coef = df_clean['RetainedJob'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (RetainedJob vs. Default): {corr_coef:.4f}")

# 5. Group by each distinct RetainedJob value to calculate default rate per job retained
grouped = (
    df_clean
    .groupby('RetainedJob')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

print("\nDefault rate by exact RetainedJob count:")
print(grouped.head(10))  # Inspect the first 10 rows

# 6. Plot default rate vs. exact RetainedJob count (scatter)
plt.figure(figsize=(8, 5))
plt.scatter(grouped['RetainedJob'], grouped['default_rate'], s=20, alpha=0.6)
plt.xlabel('Number of Jobs Retained')
plt.ylabel('Default Rate')
plt.title('Default Rate by Exact RetainedJob Count')
plt.grid(True)
plt.tight_layout()
plt.show()

bins = [-1, 0, 5, 10, 20, df_clean['RetainedJob'].max() + 1]
labels = ['0', '1–5', '6–10', '11–20', '21+']
df_clean['RetainedJob_Bin'] = pd.cut(df_clean['RetainedJob'], bins=bins, labels=labels, right=False)

binned = (
    df_clean
    .groupby('RetainedJob_Bin')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
    .dropna()
)

print("\nDefault rate by RetainedJob bins:")
print(binned)

# 8. Plot default rate by RetainedJob bins (bar chart)
plt.figure(figsize=(8, 5))
plt.bar(binned['RetainedJob_Bin'], binned['default_rate'], color='teal', alpha=0.8)
plt.xlabel('Jobs Retained (Binned)')
plt.ylabel('Default Rate')
plt.title('Default Rate by RetainedJob Bins')
plt.ylim(0, binned['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()
# Pearson correlation (RetainedJob vs. Default): 0.0124 - no significant relationship

# GuaranteeRatio
df_clean = df[['GuaranteeRatio', 'MIS_Status_Binary']].replace([pd.NA, pd.NaT, float('inf'), -float('inf')], pd.NA).dropna()

# 5. Compute Pearson correlation coefficient between GuaranteeRatio and default flag
corr_coef = df_clean['GuaranteeRatio'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (GuaranteeRatio vs. Default): {corr_coef:.4f}")

# 6. Group by distinct GuaranteeRatio values to calculate default rate
#    (If there are too many unique ratios, we will bin in the next step)
grouped_exact = (
    df_clean
    .groupby('GuaranteeRatio')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

print("\nSample of Default Rate by exact GuaranteeRatio:")
print(grouped_exact.head(10))

# 7. Scatter plot of default rate vs. exact GuaranteeRatio
plt.figure(figsize=(8, 5))
plt.scatter(grouped_exact['GuaranteeRatio'], grouped_exact['default_rate'], s=20, alpha=0.6)
plt.xlabel('GuaranteeRatio (SBA_Appv / GrAppv)')
plt.ylabel('Default Rate')
plt.title('Default Rate by Exact GuaranteeRatio')
plt.grid(True)
plt.tight_layout()
plt.show()

# 8. Bin GuaranteeRatio into ranges (e.g., 0-0.2, 0.2-0.4, ..., 0.8-1.0, >1.0 if applicable)
bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0, df_clean['GuaranteeRatio'].max()+0.01]
labels = ['0–0.2', '0.2–0.4', '0.4–0.6', '0.6–0.8', '0.8–1.0', '1.0+']
df_clean['GR_Bin'] = pd.cut(df_clean['GuaranteeRatio'], bins=bins, labels=labels, right=False)

binned = (
    df_clean
    .groupby('GR_Bin')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
    .dropna()
)

print("\nDefault Rate by GuaranteeRatio bins:")
print(binned)

# 9. Bar chart of default rate by GuaranteeRatio bins
plt.figure(figsize=(8, 5))
plt.bar(binned['GR_Bin'], binned['default_rate'], color='mediumseagreen', alpha=0.8)
plt.xlabel('GuaranteeRatio Bins')
plt.ylabel('Default Rate')
plt.title('Default Rate by GuaranteeRatio Bins')
plt.ylim(0, binned['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()


## Franchise code
df['FranchiseCode'] = pd.to_numeric(df['FranchiseCode'], errors='coerce')

# 3. Drop rows where FranchiseCode or MIS_Status_Binary is missing
df_clean = df[['FranchiseCode', 'MIS_Status_Binary']].dropna()

# 4. Compute Pearson correlation coefficient between FranchiseCode and default flag
corr_coef = df_clean['FranchiseCode'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (FranchiseCode vs. Default): {corr_coef:.4f}")

# 5. Group by each distinct FranchiseCode to calculate loan count and default rate
grouped = (
    df_clean
    .groupby('FranchiseCode')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

print("\nDefault rate by exact FranchiseCode (first 10 rows):")
print(grouped.head(10))

# 6. To reduce noise, filter to codes that appear at least N times (e.g., N = 50)
min_loans = 50
filtered = grouped[grouped['loan_count'] >= min_loans]

print(f"\nNumber of distinct FranchiseCodes with ≥ {min_loans} loans: {filtered.shape[0]}")

# 7. Identify top 10 riskiest codes (highest default_rate) among those with sufficient volume
top_risky = filtered.sort_values('default_rate', ascending=False).head(10)
print("\nTop 10 FranchiseCodes by Default Rate (loan_count ≥ 50):")
print(top_risky[['FranchiseCode', 'loan_count', 'default_rate']])

# 8. Identify top 10 safest codes (lowest default_rate) among those with sufficient volume
top_safe = filtered.sort_values('default_rate').head(10)
print("\nTop 10 FranchiseCodes by Lowest Default Rate (loan_count ≥ 50):")
print(top_safe[['FranchiseCode', 'loan_count', 'default_rate']])

# 9. Plot default rate for those top 10 riskiest FranchiseCodes
plt.figure(figsize=(8, 5))
plt.bar(
    top_risky['FranchiseCode'].astype(str),
    top_risky['default_rate'],
    color='indianred',
    alpha=0.8
)
plt.xlabel('FranchiseCode')
plt.ylabel('Default Rate')
plt.title(f'Top 10 Riskiest FranchiseCodes (≥ {min_loans} loans)')
plt.ylim(0, top_risky['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# 10. Plot default rate for those top 10 safest FranchiseCodes
plt.figure(figsize=(8, 5))
plt.bar(
    top_safe['FranchiseCode'].astype(str),
    top_safe['default_rate'],
    color='seagreen',
    alpha=0.8
)
plt.xlabel('FranchiseCode')
plt.ylabel('Default Rate')
plt.title(f'Top 10 Safest FranchiseCodes (≥ {min_loans} loans)')
plt.ylim(0, top_safe['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show() ## cungx duocj


# RevLineCr_Binary
df_clean = df[['RevLineCr_Binary', 'MIS_Status_Binary']].dropna()

# 5. Compute Pearson correlation between RevLineCr_Binary and default flag
corr_coef = df_clean['RevLineCr_Binary'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (RevLineCr_Binary vs. Default): {corr_coef:.4f}")

# 6. Calculate default rate by RevLineCr category
grouped = (
    df_clean
    .groupby('RevLineCr_Binary')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)
grouped['category'] = grouped['RevLineCr_Binary'].map({0: 'Term Loan', 1: 'Revolving Line'})

print("\nDefault rate by RevLineCr category:")
print(grouped[['category', 'loan_count', 'default_rate']])

# 7. Plot default rate by RevLineCr category
plt.figure(figsize=(6, 4))
plt.bar(
    grouped['category'],
    grouped['default_rate'],
    color=['skyblue', 'salmon'],
    alpha=0.8
)
plt.xlabel('Loan Type')
plt.ylabel('Default Rate')
plt.title('Default Rate: Term Loan vs. Revolving Line of Credit')
plt.ylim(0, grouped['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# LowDoc Binary
df_clean = df[['LowDoc_Binary', 'MIS_Status_Binary']].dropna()

# 5. Compute Pearson correlation between LowDoc_Binary and default flag
corr_coef = df_clean['LowDoc_Binary'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (LowDoc_Binary vs. Default): {corr_coef:.4f}")

# 6. Calculate default rate by LowDoc category
grouped = (
    df_clean
    .groupby('LowDoc_Binary')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)
grouped['category'] = grouped['LowDoc_Binary'].map({0: 'Standard Documentation', 1: 'Low Documentation'})

print("\nDefault rate by LowDoc category:")
print(grouped[['category', 'loan_count', 'default_rate']])

# 7. Plot default rate by LowDoc category
plt.figure(figsize=(6, 4))
plt.bar(
    grouped['category'],
    grouped['default_rate'],
    color=['steelblue', 'tomato'],
    alpha=0.8
)
plt.xlabel('Documentation Type')
plt.ylabel('Default Rate')
plt.title('Default Rate: Standard vs. Low-Documentation Loans')
plt.ylim(0, grouped['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Approval FY
df['ApprovalFY'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')
df_year = df[df['ApprovalFY'].notna()].copy()

# Group by Fiscal Year and compute default rate
yearly_stats = (
    df_year
    .groupby('ApprovalFY')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

# Plot default rate over years
plt.figure(figsize=(12, 6))
plt.plot(yearly_stats['ApprovalFY'], yearly_stats['default_rate'], marker='o', linestyle='-')
plt.title('Default Rate by Fiscal Year of Loan Approval')
plt.xlabel('Fiscal Year')
plt.ylabel('Default Rate')
plt.grid(True)
plt.tight_layout()
plt.show()

df['ApprovalFY'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')
print(df['ApprovalFY'].describe())

print("Unique values in ApprovalFY:", df['ApprovalFY'].unique())
print("Minimum ApprovalFY:", df['ApprovalFY'].min())
print("Maximum ApprovalFY:", df['ApprovalFY'].max())

# Check initial number of rows
initial_count = df.shape[0]

# Drop rows with ApprovalFY > 2025
df['ApprovalFY'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')
df_cleaned = df[df['ApprovalFY'] <= 2025]

# Check new number of rows
final_count = df_cleaned.shape[0]

# Print summary
dropped = initial_count - final_count
print(f"Initial rows: {initial_count}")
print(f"Retained rows: {final_count}")
print(f"Dropped rows: {dropped}")

# Initial rows: 899164
# Retained rows: 898993
# Dropped rows: 171

print("Unique values in ApprovalFY:", df_cleaned['ApprovalFY'].unique())
print("Minimum ApprovalFY:", df_cleaned['ApprovalFY'].min())
print("Maximum ApprovalFY:", df_cleaned['ApprovalFY'].max())

# Create InflationRate
df['ApprovalFY_int'] = df['ApprovalFY'].astype(int)


# Minimum ApprovalFY: 1962.0
# Maximum ApprovalFY: 2014.0

inflation_dict = {
    1962: 1.30, 1963: 1.60, 1964: 1.00, 1965: 1.90, 1966: 3.50, 1967: 3.00, 1968: 4.70,
    1969: 6.20, 1970: 5.60, 1971: 3.30, 1972: 3.40, 1973: 8.70, 1974: 12.30, 1975: 6.90,
    1976: 4.90, 1977: 6.70, 1978: 9.00, 1979: 13.30, 1980: 12.50, 1981: 8.90, 1982: 3.80,
    1983: 3.80, 1984: 3.90, 1985: 3.80, 1986: 1.10, 1987: 4.40, 1988: 4.40, 1989: 4.60,
    1990: 6.10, 1991: 3.10, 1992: 2.90, 1993: 2.70, 1994: 2.70, 1995: 2.50, 1996: 3.30,
    1997: 1.70, 1998: 1.60, 1999: 2.70, 2000: 3.40, 2001: 1.60, 2002: 2.40, 2003: 1.90,
    2004: 3.30, 2005: 3.40, 2006: 2.50, 2007: 4.10, 2008: 0.10, 2009: 2.70, 2010: 1.50,
    2011: 3.00, 2012: 1.70, 2013: 1.50, 2014: 0.80
}



df_cleaned['InflationRate'] = df_cleaned['ApprovalFY'].map(inflation_dict)

print(df_cleaned[['ApprovalFY', 'InflationRate']].dropna().head())

# GDP Growth
gdp_growth = {
    1962: 6.10,
    1963: 4.40,
    1964: 5.80,
    1965: 6.40,
    1966: 6.50,
    1967: 2.50,
    1968: 4.80,
    1969: 3.10,
    1970: 0.22,
    1971: 3.29,
    1972: 5.26,
    1973: 5.65,
    1974: -0.54,
    1975: -0.21,
    1976: 5.39,
    1977: 4.62,
    1978: 5.54,
    1979: 3.17,
    1980: -0.26,
    1981: 2.54,
    1982: -1.80,
    1983: 4.58,
    1984: 7.24,
    1985: 4.17,
    1986: 3.46,
    1987: 3.45,
    1988: 4.18,
    1989: 3.67,
    1990: 1.89,
    1991: -0.11,
    1992: 3.52,
    1993: 2.75,
    1994: 4.03,
    1995: 2.68,
    1996: 3.77,
    1997: 4.45,
    1998: 4.48,
    1999: 4.79,
    2000: 4.08,
    2001: 0.96,
    2002: 1.70,
    2003: 2.80,
    2004: 3.85,
    2005: 3.48,
    2006: 2.78,
    2007: 2.00,
    2008: 0.11,
    2009: -2.58,
    2010: 2.70,
    2011: 1.56,
    2012: 2.29,
    2013: 2.12,
    2014: 2.52,
}

df_cleaned['GDPGrowth'] = df_cleaned['ApprovalFY'].astype(int).map(gdp_growth)


# Interest rate
interest_rate = {
    1961: 3.11,
    1962: 3.22,
    1963: 3.37,
    1964: 2.95,
    1965: 2.57,
    1966: 2.65,
    1967: 2.41,
    1968: 1.86,
    1969: 2.85,
    1970: 2.51,
    1971: 0.62,
    1972: 0.88,
    1973: 2.41,
    1974: 1.65,
    1975: -1.28,
    1976: 1.27,
    1977: 0.58,
    1978: 1.89,
    1979: 4.03,
    1980: 5.72,
    1981: 8.59,
    1982: 8.18,
    1983: 6.62,
    1984: 8.14,
    1985: 6.56,
    1986: 6.19,
    1987: 5.59,
    1988: 5.59,
    1989: 6.69,
    1990: 6.04,
    1991: 4.92,
    1992: 3.88,
    1993: 3.55,
    1994: 4.90,
    1995: 6.59,
    1996: 6.32,
    1997: 6.60,
    1998: 7.15,
    1999: 6.49,
    2000: 6.81,
    2001: 4.57,
    2002: 3.07,
    2003: 2.11,
    2004: 1.61,
    2005: 2.96,
    2006: 4.73,
    2007: 5.20,
    2008: 3.10,
    2009: 2.62,
    2010: 2.01,
    2011: 1.16,
    2012: 1.36,
    2013: 1.52,
    2014: 1.48,
}

df_cleaned['InterestRate'] = df_cleaned['ApprovalFY'].astype(int).map(interest_rate)

# Define the final feature set (drop everything else)
features = [
    'Term',
    'DisbursementGross',
    'GuaranteeRatio',
    'FranchiseCode',
    'RevLineCr_Binary',
    'LowDoc_Binary',
    'UrbanRural_Numeric',
    'RealEstate',
    'Recession',
    'NAICS2',
    'State',
    'ApprovalFY', 
    'InterestRate',
    'GDPGrowth',
    'InflationRate']

target = 'MIS_Status_Binary'

# Create modeling DataFrame
df_model = df_cleaned[features + [target]].copy()

# Drop rows with NA in features or target
df_model.replace([np.inf, -np.inf], np.nan, inplace=True)
df_model.dropna(inplace=True)

# Identify categorical columns (to be one-hot encoded)
categorical_cols = [
    'FranchiseCode',
    'RevLineCr_Binary',
    'LowDoc_Binary',
    'UrbanRural_Numeric',
    'RealEstate',
    'Recession',
    'NAICS2',
    'State',
    'ApprovalFY'
]

df_model[categorical_cols] = df_model[categorical_cols].astype(str)

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df_model, columns=categorical_cols, drop_first=True)

# Split into features (X) and target (y)
X = df_encoded.drop(target, axis=1)
y = df_encoded[target]
disb = df_encoded['DisbursementGross'].values.copy()

X_train, X_test, y_train, y_test, disb_train, disb_test = train_test_split(
    X, y, disb,
    test_size=0.90,
    random_state=1,
    stratify=y
)

print("X_train.shape:", X_train.shape)  
print("X_test.shape: ", X_test.shape)  

numeric_cols = [
    'Term',
    'DisbursementGross',
    'GuaranteeRatio',
    'InflationRate',
    'InterestRate',
    'GDPGrowth']

all_cols = X_train.columns.tolist()
categorical_dummy_cols = [c for c in all_cols if c not in numeric_cols]

from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('passthrough', 'passthrough', categorical_dummy_cols)],
    remainder='drop')

# -------------------------------
# 8) Define Ridge and Lasso pipelines

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    confusion_matrix,
    roc_curve,
    auc
)
import matplotlib.pyplot as plt
ridge_pipe = Pipeline([
    ('preproc', preprocessor),
    ('clf', LogisticRegression(
        penalty='l2',
        solver='saga',
        max_iter=5000,
        random_state=42
    ))
])

lasso_pipe = Pipeline([
    ('preproc', preprocessor),
    ('clf', LogisticRegression(
        penalty='l1',
        solver='saga',
        max_iter=5000,
        random_state=42
    ))
])

# 9) Parameter distributions for RandomizedSearchCV
param_dist = {
    'clf__C': np.logspace(-4, 4, 20).tolist(),
    'clf__class_weight': [None, 'balanced']
}

# 10) RandomizedSearchCV for Ridge
search_ridge = RandomizedSearchCV(
    estimator=ridge_pipe,
    param_distributions=param_dist,
    n_iter=20,
    scoring='roc_auc',
    cv=5,
    verbose=2,
    random_state=42,
    n_jobs=-1,
    refit=True
)
search_ridge.fit(X_train, y_train)
best_ridge = search_ridge.best_estimator_

print("=== RIDGE (ℓ₂) LOGISTIC REGRESSION ===")
print("Best hyperparameters:", search_ridge.best_params_)
print("Best CV ROC AUC:   ", search_ridge.best_score_)

# 11) RandomizedSearchCV for Lasso
search_lasso = RandomizedSearchCV(
    estimator=lasso_pipe,
    param_distributions=param_dist,
    n_iter=20,
    scoring='roc_auc',
    cv=5,
    verbose=2,
    random_state=42,
    n_jobs=-1,
    refit=True
)
search_lasso.fit(X_train, y_train)
best_lasso = search_lasso.best_estimator_

print("\n=== LASSO (ℓ₁) LOGISTIC REGRESSION ===")
print("Best hyperparameters:", search_lasso.best_params_)
print("Best CV ROC AUC:   ", search_lasso.best_score_)

# ==============================================
# 12) Define net_profit & tune_threshold helpers
# ==============================================
def net_profit(y_true, y_pred, disbursement):
    """
    - y_true[i]: 0 = PIF, 1 = Default
    - y_pred[i]: 0 = Grant loan, 1 = Deny loan
    - disbursement[i]: DisbursementGross
      grant + PIF     → +5% * amt
      grant + Default → −25% * amt
      deny            → 0
    """
    profit = 0.0
    for truth, pred, amt in zip(y_true, y_pred, disbursement):
        if pred == 0:
            if truth == 0:
                profit += 0.05 * amt
            else:
                profit -= 0.25 * amt
        else:
            if truth ==0:
                profit -= 0.05*amt
    return profit

def tune_threshold(y_true, prob_default, disbursement, plot=True):
    thresholds = np.arange(0.00, 1.00, 0.01)
    profits = []
    for t in thresholds:
        y_pred = (prob_default >= t).astype(int)
        profits.append(net_profit(y_true, y_pred, disbursement))
    profits = np.array(profits)
    best_idx       = np.nanargmax(profits)
    best_threshold = thresholds[best_idx]
    max_profit     = profits[best_idx]
    if plot:
        plt.figure(figsize=(8,4))
        plt.plot(thresholds, profits, marker='o', color='orange', label='Net Profit')
        plt.axvline(x=best_threshold, color='red', linestyle='--',
                    label=f'Best Threshold = {best_threshold:.2f}')
        plt.title("Threshold vs Net Profit")
        plt.xlabel("Threshold for P(Default)")
        plt.ylabel("Total Net Profit (USD)")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()
    return best_threshold, max_profit

# ==============================================
# 13) Evaluate Ridge on the TEST set
# ==============================================
print("\n=== EVALUATION ON TEST SET (RIDGE) ===")

probs_default_ridge = best_ridge.predict_proba(X_test)[:, 1]

# 13.1) Classification report @ threshold = 0.5
y_pred_05_ridge = (probs_default_ridge < 0.5).astype(int)  # 0=grant if P(Default)<0.5
print("\n-- Classification Report (threshold = 0.5) --")
print(classification_report(y_test, y_pred_05_ridge, digits=4))
print("Confusion Matrix (threshold = 0.5):")
print(confusion_matrix(y_test, y_pred_05_ridge))
print("Test ROC AUC:", roc_auc_score(y_test, probs_default_ridge))

# 13.2) Net Profit @ threshold = 0.5
profit_05_ridge = net_profit(y_test.values, y_pred_05_ridge, disb_test)
print(f"Net Profit (Ridge) at threshold = 0.5: ${profit_05_ridge:,.2f}")

# 13.3) ROC Curve
fpr_r, tpr_r, _ = roc_curve(y_test, probs_default_ridge)
roc_auc_r = auc(fpr_r, tpr_r)
plt.figure(figsize=(6,4))
plt.plot(fpr_r, tpr_r, color='blue', lw=2, label=f'AUC = {roc_auc_r:.4f}')
plt.plot([0,1],[0,1],'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Ridge Logistic')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

# 13.4) Find optimal threshold for Ridge
best_thresh_ridge, best_profit_ridge = tune_threshold(
    y_true=y_test.values,
    prob_default=probs_default_ridge,
    disbursement=disb_test,
    plot=True
)
print(f"Optimal Threshold (Ridge): {best_thresh_ridge:.2f}")
print(f"Max Net Profit    (Ridge): ${best_profit_ridge:,.2f}")

# 13.5) Classification report @ optimal threshold
y_pred_opt_ridge = (probs_default_ridge >= best_thresh_ridge).astype(int)
print("\n-- Classification Report (optimized threshold) --")
print(classification_report(y_test, y_pred_opt_ridge, digits=4))
print("Confusion Matrix (optimized threshold):")
print(confusion_matrix(y_test, y_pred_opt_ridge))

# ==============================================
# 14) Evaluate Lasso on the TEST set
# ==============================================
print("\n=== EVALUATION ON TEST SET (LASSO) ===")

probs_default_lasso = best_lasso.predict_proba(X_test)[:, 1]

# 14.1) Classification report @ threshold = 0.5
y_pred_05_lasso = (probs_default_lasso < 0.5).astype(int)
print("\n-- Classification Report (threshold = 0.5) --")
print(classification_report(y_test, y_pred_05_lasso, digits=4))
print("Confusion Matrix (threshold = 0.5):")
print(confusion_matrix(y_test, y_pred_05_lasso))
print("Test ROC AUC:", roc_auc_score(y_test, probs_default_lasso))

# 14.2) Net Profit @ threshold = 0.5
profit_05_lasso = net_profit(y_test.values, y_pred_05_lasso, disb_test)
print(f"Net Profit (Lasso) at threshold = 0.5: ${profit_05_lasso:,.2f}")

# 14.3) ROC Curve
fpr_l, tpr_l, _ = roc_curve(y_test, probs_default_lasso)
roc_auc_l = auc(fpr_l, tpr_l)
plt.figure(figsize=(6,4))
plt.plot(fpr_l, tpr_l, color='green', lw=2, label=f'AUC = {roc_auc_l:.4f}')
plt.plot([0,1],[0,1],'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Lasso Logistic')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

# 14.4) Find optimal threshold for Lasso
best_thresh_lasso, best_profit_lasso = tune_threshold(
    y_true=y_test.values,
    prob_default=probs_default_lasso,
    disbursement=disb_test,
    plot=True
)
print(f"Optimal Threshold (Lasso): {best_thresh_lasso:.2f}")
print(f"Max Net Profit    (Lasso): ${best_profit_lasso:,.2f}")

# 14.5) Classification report @ optimal threshold
y_pred_opt_lasso = (probs_default_lasso >= best_thresh_lasso).astype(int)
print("\n-- Classification Report (optimized threshold) --")
print(classification_report(y_test, y_pred_opt_lasso, digits=4))
print("Confusion Matrix (optimized threshold):")
print(confusion_matrix(y_test, y_pred_opt_lasso))

# ==============================================
# 15) Compute Rate of Return (ROI) for both models
# ==============================================
def rate_of_return(net_profit_val, disbursement, y_pred):
    """
    ROI = net_profit / (Tổng Disbursement của các khoản được grant)
    """
    granted_mask = (y_pred == 0)
    total_granted_disb = disbursement[granted_mask].sum()
    if total_granted_disb == 0:
        return np.nan
    return net_profit_val / total_granted_disb

# 15.1) Ridge ROI
roi_05_ridge = rate_of_return(
    net_profit_val=profit_05_ridge,
    disbursement=disb_test,
    y_pred=y_pred_05_ridge
)
print(f"\nROI (Ridge) at threshold = 0.5: {roi_05_ridge:.4%}")

roi_opt_ridge = rate_of_return(
    net_profit_val=best_profit_ridge,
    disbursement=disb_test,
    y_pred=y_pred_opt_ridge
)
print(f"ROI (Ridge) at optimal threshold = {best_thresh_ridge:.2f}: {roi_opt_ridge:.4%}")

# 15.2) Lasso ROI
roi_05_lasso = rate_of_return(
    net_profit_val=profit_05_lasso,
    disbursement=disb_test,
    y_pred=y_pred_05_lasso
)
print(f"\nROI (Lasso) at threshold = 0.5: {roi_05_lasso:.4%}")

roi_opt_lasso = rate_of_return(
    net_profit_val=best_profit_lasso,
    disbursement=disb_test,
    y_pred=y_pred_opt_lasso
)
print(f"ROI (Lasso) at optimal threshold = {best_thresh_lasso:.2f}: {roi_opt_lasso:.4%}")







import joblib

ridge_filename = 'ridge_model.joblib'
lasso_filename = 'lasso_model.joblib'

joblib.dump(best_ridge, ridge_filename)
print(f"Saved Ridge model to {ridge_filename}")

joblib.dump(best_lasso, lasso_filename)
print(f"Saved Lasso model to {lasso_filename}")



# Load Ridge:
loaded_ridge = joblib.load(ridge_filename)

# Load Lasso:
loaded_lasso = joblib.load(lasso_filename)

print("Ridge loaded, test AUC on X_test:", 
      roc_auc_score(y_test, loaded_ridge.predict_proba(X_test)[:,1]))
print("Lasso loaded, test AUC on X_test:", 
      roc_auc_score(y_test, loaded_lasso.predict_proba(X_test)[:,1]))












































