# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1udnPt0mfu7hlfQBEe8B4SclP9eQ7IoVN

1. **Load and Run correlation matrix**
"""


import pandas as pd
import matplotlib.pyplot as plt

# 1) Load
df = pd.read_csv('SBAnational.csv')
df.head()
print(df.dtypes)

# Clean currency columns
currency_cols = ['DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv', 'ChgOffPrinGr']
for col in currency_cols:
    df[col] = pd.to_numeric(df[col].astype(str).str.replace('[\$,]', '', regex=True), errors='coerce')

# Parse datetime columns
df['ApprovalDate'] = pd.to_datetime(df['ApprovalDate'], errors='coerce')
df['DisbursementDate'] = pd.to_datetime(df['DisbursementDate'], errors='coerce')
df['ChgOffDate'] = pd.to_datetime(df['ChgOffDate'], errors='coerce')

# Target variable
df['MIS_Status_Binary'] = df['MIS_Status'].map({'P I F': 0, 'CHGOFF': 1})
df['LowDoc_Binary'] = df['LowDoc'].map({'Y': 1, 'N': 0})
df['RevLineCr_Binary'] = df['RevLineCr'].map({'Y': 1, 'N': 0})
df['UrbanRural_Numeric'] = pd.to_numeric(df['UrbanRural'], errors='coerce')

numeric_features = [
    'Term', 'NoEmp', 'CreateJob', 'RetainedJob',
    'FranchiseCode', 'DisbursementGross', 'BalanceGross',
    'ChgOffPrinGr', 'GrAppv', 'SBA_Appv',
    'LowDoc_Binary', 'RevLineCr_Binary', 'UrbanRural_Numeric',
    'MIS_Status_Binary'
]

df_corr = df[numeric_features].dropna()

# Compute correlation matrix
correlation_matrix = df_corr.corr()
print(correlation_matrix)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Selected Numeric Features", fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

import numpy as np

# Mask to get only upper triangle, to avoid duplicate pairs
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)

# Extract correlation values from upper triangle
corr_pairs = correlation_matrix.where(mask).stack()

# Filter pairs with correlation > 0.4
strong_corr = corr_pairs[corr_pairs > 0.4]

print("Correlation pairs with correlation > 0.4:")
print(strong_corr)

"""2. **New vs Established Business**"""

df['NewExist'].value_counts()

df_filtered = df[df['NewExist'].isin([1, 2])]

default_rates = df_filtered.groupby('NewExist')['MIS_Status_Binary'].mean()

business_type_map = {1: 'New', 2: 'Established'}
default_rates.index = default_rates.index.map(business_type_map)

print("Default rate by business status:")
print(default_rates)

# Plot default rates
default_rates.plot(kind='bar', color=['orange', 'skyblue'])
plt.ylabel('Default Rate')
plt.title('Default Rate: New vs Established Businesses')
plt.xticks(rotation=0)
plt.show()

"""The default rate of 2 business models does not differ much, suggesting that business longevity would not be a good indicator for default rate

3. **Loans Backed by Real Estate**
"""

df['RealEstate']=df['Term'].apply(lambda x: 1 if x >= 240 else 0)
df['RealEstate'].value_counts()

grouped = df.groupby('RealEstate')['MIS_Status_Binary']
default_rate = grouped.mean()
pif_rate     = 1 - default_rate

rates = pd.DataFrame({
    'Default Rate': default_rate,
    'Pay-in-Full Rate': pif_rate
}).rename(index={0: 'Non Real Estate Backed (<240m)', 1: 'Real Estate Backed (≥240m)'})

print(rates)

rates.plot(kind='bar', figsize=(8,5))
plt.ylim(0,1)
plt.ylabel('Rate')
plt.title('Default vs. Pay-in-Full Rates\nby Real-Estate Backing')
plt.xticks(rotation=0)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

"""Loans backed by real estate exhibit a dramatically lower default rate (1.6% vs. 20.8%) and a correspondingly higher pay-in-full rate (98.4% vs. 79.2%). This aligns perfectly with the collateral hypothesis: long-term, real-estate-secured loans carry far less risk of charge-off because the underlying property value typically covers outstanding principal

4. **Economic Recession**
"""

recession_start = pd.to_datetime('2007-12-01')
recession_end   = pd.to_datetime('2009-06-30')

# Make sure ApprovalDate is datetime and drop rows missing it
df = df[df['ApprovalDate'].notna()]

# Create Recession dummy:
df['Recession'] = df['ApprovalDate'].between(recession_start, recession_end).astype(int)
print(df['Recession'].value_counts())

# Compute default and pay-in-full rates by Recession flag
grouped = df.groupby('Recession')['MIS_Status_Binary']
default_rate = grouped.mean()            # mean of 1/0 default flags
pif_rate     = 1 - default_rate

rates = pd.DataFrame({
    'Default Rate': default_rate,
    'Pay-in-Full Rate': pif_rate
}).rename(index={0: 'Outside Recession', 1: 'During Recession'})

print(rates)

ax = rates.plot(
    kind='bar',
    stacked=True,
    figsize=(8, 5)
)

ax.set_ylabel('Proportion')
ax.set_ylim(0, 1)
ax.set_title('Loan Outcomes by Recession Period (Stacked)')
ax.legend(loc='upper right')

for container in ax.containers:
    ax.bar_label(container, fmt='%.3f', label_type='center')

plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

"""- The patter is clear, during recession companies have higher default rate and lower PIF. Therefore, it is suggested that loan performance is highly sensitive to macroeconomic conditions. Underwriting standards that look sound in growth periods can yield sharply higher losses in downturns
- Regulators or risk managers should stress‐test portfolios against recession-like scenarios—SBA portfolios could see defaults more than double under severe downturns

5. **SBA’s Guaranteed Portion of Approved Loan**
"""

df_sba = df[['SBA_Appv', 'MIS_Status']].dropna()
df_sba = df[df['MIS_Status'].isin(['P I F', 'CHGOFF'])]

# Visualize using a boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_sba, x='MIS_Status', y='SBA_Appv')
plt.title('Distribution of SBA Guaranteed Amount by Loan Status')
plt.xlabel('MIS_Status (P I F = Paid in Full, CHGOFF = Charged Off)')
plt.ylabel('SBA Guaranteed Amount ($)')
plt.grid(True)
plt.show()

"""- The median SBA guaranteed amount is higher for PIF loans than for CHGOFF loans --> Suggests that loans with higher guaranteed amounts are more likely to be paid back in full

- The IQR has the box height for PIF is larger, showing more variability among successfully repaid loans --> PIF have more outliers while Charge Off have less, indicating less success rate when the SBA's loan proportion covered is less

6. **Default Rate by Industry, State**
"""

state_stats = df.groupby('State').agg(
    loan_count      = ('MIS_Status_Binary','size'),
    default_rate    = ('MIS_Status_Binary','mean')
).reset_index()

top10 = state_stats.sort_values('default_rate', ascending=False).head(10)

print("Top 10 States by SBA Default Rate :")
print(top10[['State','loan_count','default_rate']])

plt.bar(top10['State'], top10['default_rate'])
plt.ylim(0, top10['default_rate'].max() * 1.1)
plt.ylabel('Default Rate')
plt.xlabel('State')
plt.title('Top 10 States by SBA Loan Default Rate\n')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""FL has by far the highest default rate at 27.4%, nearly 4 points above the next state. This suggests that, during your sample period, SBA‐backed loans in Florida carried substantially more risk than in other large states"""

df.columns

df['NAICS2'] = df['NAICS'].astype(int).astype(str).str[:2]

# Compute counts and default rates by sector
industry_stats = (
    df
      .groupby('NAICS2')['MIS_Status_Binary']
      .agg(loan_count='size', default_rate='mean')
      .reset_index()
)


# Sort and take top 10 riskiest sectors
top10_ind = industry_stats.sort_values('default_rate', ascending=False).head(10)

print("Top 10 NAICS 2-digit Sectors by Default Rate")
print(top10_ind)

# Plot
fig, ax = plt.subplots(figsize=(10,6))
bars = ax.bar(top10_ind['NAICS2'], top10_ind['default_rate'] * 100, color='teal')

# Label each bar with percent
ax.bar_label(bars, labels=[f"{x:2f}%" for x in top10_ind['default_rate'] * 100], padding=4)

ax.set_ylim(0, top10_ind['default_rate'].max() * 110)
ax.set_ylabel('Default Rate (%)')
ax.set_xlabel('NAICS 2-Digit Sector')
ax.set_title(f'Top 10 SBA Default Rates by Industry Sector\n(NAICS 2-Digits)')
plt.tight_layout()
plt.show()

"""| NAICS 2-Digit | Sector                                      | Loan Count | Default Rate |
| ------------: | ------------------------------------------- | ---------: | -----------: |
|            53 | Real Estate & Rental & Leasing              |     13,632 |        28.7% |
|            52 | Finance & Insurance                         |      9,496 |        28.4% |
|            48 | Transportation & Warehousing                |     20,310 |        26.9% |
|            51 | Information                                 |     11,379 |        24.8% |
|            61 | Educational Services                        |      6,425 |        24.2% |
|            56 | Administrative & Support & Waste Management |     32,685 |        23.6% |
|            45 | Retail Trade                                |     42,514 |        23.4% |
|            23 | Construction                                |     66,646 |        23.3% |
|            49 | Transit & Ground Passenger Transportation   |      2,221 |        23.0% |
|            44 | Retail Trade (second sub-category)          |     84,737 |        22.4% |

"""

df.columns

"""**TRAIN THE MODEL**

# Feature selection and define X and y**
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, make_scorer

# Recompute engineered feature
df['GuaranteeRatio'] = df['SBA_Appv'] / df['GrAppv']

# Check for NoEmployee
df['NoEmp'] = pd.to_numeric(df['NoEmp'], errors='coerce')

# 3. Drop rows where NoEmp or MIS_Status_Binary is missing
df_clean = df[['NoEmp', 'MIS_Status_Binary']].dropna()

# 4. Compute Pearson correlation coefficient between NoEmp and default flag
corr_coef = df_clean['NoEmp'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (NoEmp vs. Default): {corr_coef:.4f}")

# 5. Calculate default rate by each distinct employee count
#    (If there are too many distinct employee counts, consider binning instead.)
grouped = df_clean.groupby('NoEmp')['MIS_Status_Binary'].agg(
    loan_count='size',
    default_rate='mean'
).reset_index()

plt.figure(figsize=(8, 5))
plt.scatter(grouped['NoEmp'], grouped['default_rate'], s=20, alpha=0.6)
plt.xlabel('Number of Employees (NoEmp)')
plt.ylabel('Default Rate')
plt.title('Default Rate by Exact Number of Employees')
plt.grid(True)
plt.tight_layout()
plt.show()

# Check for 
df['CreateJob'] = pd.to_numeric(df['CreateJob'], errors='coerce')

# 3. Drop rows where CreateJob or MIS_Status_Binary is missing
df_clean = df[['CreateJob', 'MIS_Status_Binary']].dropna()

# 4. Compute Pearson correlation coefficient between CreateJob and default flag
corr_coef = df_clean['CreateJob'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (CreateJob vs. Default): {corr_coef:.4f}")

# 5. Group by each distinct CreateJob value to calculate default rate per job created
grouped = (
    df_clean
    .groupby('CreateJob')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

print("\nDefault rate by exact CreateJob count:")
print(grouped.head(10))  # print first 10 rows for inspection

# 6. Plot default rate vs. exact CreateJob count (scatter)
plt.figure(figsize=(8, 5))
plt.scatter(grouped['CreateJob'], grouped['default_rate'], s=20, alpha=0.6)
plt.xlabel('Number of Jobs Created (CreateJob)')
plt.ylabel('Default Rate')
plt.title('Default Rate by Exact CreateJob Count')
plt.grid(True)
plt.tight_layout()
plt.show()

bins = [-1, 0, 5, 10, 20, df_clean['CreateJob'].max()+1]
labels = ['0', '1–5', '6–10', '11–20', '21+']
df_clean['CreateJob_Bin'] = pd.cut(df_clean['CreateJob'], bins=bins, labels=labels, right=False)

grouped = (
    df_clean
    .groupby('CreateJob_Bin')['MIS_Status_Binary']
    .mean()
    .reset_index()
)
print(grouped)
## No significant relationship

# Retained Job
df['RetainedJob'] = pd.to_numeric(df['RetainedJob'], errors='coerce')

# 3. Drop rows where RetainedJob or MIS_Status_Binary is missing
df_clean = df[['RetainedJob', 'MIS_Status_Binary']].dropna()

# 4. Compute Pearson correlation coefficient between RetainedJob and default flag
corr_coef = df_clean['RetainedJob'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (RetainedJob vs. Default): {corr_coef:.4f}")

# 5. Group by each distinct RetainedJob value to calculate default rate per job retained
grouped = (
    df_clean
    .groupby('RetainedJob')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

print("\nDefault rate by exact RetainedJob count:")
print(grouped.head(10))  # Inspect the first 10 rows

# 6. Plot default rate vs. exact RetainedJob count (scatter)
plt.figure(figsize=(8, 5))
plt.scatter(grouped['RetainedJob'], grouped['default_rate'], s=20, alpha=0.6)
plt.xlabel('Number of Jobs Retained')
plt.ylabel('Default Rate')
plt.title('Default Rate by Exact RetainedJob Count')
plt.grid(True)
plt.tight_layout()
plt.show()

bins = [-1, 0, 5, 10, 20, df_clean['RetainedJob'].max() + 1]
labels = ['0', '1–5', '6–10', '11–20', '21+']
df_clean['RetainedJob_Bin'] = pd.cut(df_clean['RetainedJob'], bins=bins, labels=labels, right=False)

binned = (
    df_clean
    .groupby('RetainedJob_Bin')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
    .dropna()
)

print("\nDefault rate by RetainedJob bins:")
print(binned)

# 8. Plot default rate by RetainedJob bins (bar chart)
plt.figure(figsize=(8, 5))
plt.bar(binned['RetainedJob_Bin'], binned['default_rate'], color='teal', alpha=0.8)
plt.xlabel('Jobs Retained (Binned)')
plt.ylabel('Default Rate')
plt.title('Default Rate by RetainedJob Bins')
plt.ylim(0, binned['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()
# Pearson correlation (RetainedJob vs. Default): 0.0124 - no significant relationship

# GuaranteeRatio
df_clean = df[['GuaranteeRatio', 'MIS_Status_Binary']].replace([pd.NA, pd.NaT, float('inf'), -float('inf')], pd.NA).dropna()

# 5. Compute Pearson correlation coefficient between GuaranteeRatio and default flag
corr_coef = df_clean['GuaranteeRatio'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (GuaranteeRatio vs. Default): {corr_coef:.4f}")

# 6. Group by distinct GuaranteeRatio values to calculate default rate
#    (If there are too many unique ratios, we will bin in the next step)
grouped_exact = (
    df_clean
    .groupby('GuaranteeRatio')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

print("\nSample of Default Rate by exact GuaranteeRatio:")
print(grouped_exact.head(10))

# 7. Scatter plot of default rate vs. exact GuaranteeRatio
plt.figure(figsize=(8, 5))
plt.scatter(grouped_exact['GuaranteeRatio'], grouped_exact['default_rate'], s=20, alpha=0.6)
plt.xlabel('GuaranteeRatio (SBA_Appv / GrAppv)')
plt.ylabel('Default Rate')
plt.title('Default Rate by Exact GuaranteeRatio')
plt.grid(True)
plt.tight_layout()
plt.show()

# 8. Bin GuaranteeRatio into ranges (e.g., 0-0.2, 0.2-0.4, ..., 0.8-1.0, >1.0 if applicable)
bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0, df_clean['GuaranteeRatio'].max()+0.01]
labels = ['0–0.2', '0.2–0.4', '0.4–0.6', '0.6–0.8', '0.8–1.0', '1.0+']
df_clean['GR_Bin'] = pd.cut(df_clean['GuaranteeRatio'], bins=bins, labels=labels, right=False)

binned = (
    df_clean
    .groupby('GR_Bin')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
    .dropna()
)

print("\nDefault Rate by GuaranteeRatio bins:")
print(binned)

# 9. Bar chart of default rate by GuaranteeRatio bins
plt.figure(figsize=(8, 5))
plt.bar(binned['GR_Bin'], binned['default_rate'], color='mediumseagreen', alpha=0.8)
plt.xlabel('GuaranteeRatio Bins')
plt.ylabel('Default Rate')
plt.title('Default Rate by GuaranteeRatio Bins')
plt.ylim(0, binned['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()


## Franchise code
df['FranchiseCode'] = pd.to_numeric(df['FranchiseCode'], errors='coerce')

# 3. Drop rows where FranchiseCode or MIS_Status_Binary is missing
df_clean = df[['FranchiseCode', 'MIS_Status_Binary']].dropna()

# 4. Compute Pearson correlation coefficient between FranchiseCode and default flag
corr_coef = df_clean['FranchiseCode'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (FranchiseCode vs. Default): {corr_coef:.4f}")

# 5. Group by each distinct FranchiseCode to calculate loan count and default rate
grouped = (
    df_clean
    .groupby('FranchiseCode')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

print("\nDefault rate by exact FranchiseCode (first 10 rows):")
print(grouped.head(10))

# 6. To reduce noise, filter to codes that appear at least N times (e.g., N = 50)
min_loans = 50
filtered = grouped[grouped['loan_count'] >= min_loans]

print(f"\nNumber of distinct FranchiseCodes with ≥ {min_loans} loans: {filtered.shape[0]}")

# 7. Identify top 10 riskiest codes (highest default_rate) among those with sufficient volume
top_risky = filtered.sort_values('default_rate', ascending=False).head(10)
print("\nTop 10 FranchiseCodes by Default Rate (loan_count ≥ 50):")
print(top_risky[['FranchiseCode', 'loan_count', 'default_rate']])

# 8. Identify top 10 safest codes (lowest default_rate) among those with sufficient volume
top_safe = filtered.sort_values('default_rate').head(10)
print("\nTop 10 FranchiseCodes by Lowest Default Rate (loan_count ≥ 50):")
print(top_safe[['FranchiseCode', 'loan_count', 'default_rate']])

# 9. Plot default rate for those top 10 riskiest FranchiseCodes
plt.figure(figsize=(8, 5))
plt.bar(
    top_risky['FranchiseCode'].astype(str),
    top_risky['default_rate'],
    color='indianred',
    alpha=0.8
)
plt.xlabel('FranchiseCode')
plt.ylabel('Default Rate')
plt.title(f'Top 10 Riskiest FranchiseCodes (≥ {min_loans} loans)')
plt.ylim(0, top_risky['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# 10. Plot default rate for those top 10 safest FranchiseCodes
plt.figure(figsize=(8, 5))
plt.bar(
    top_safe['FranchiseCode'].astype(str),
    top_safe['default_rate'],
    color='seagreen',
    alpha=0.8
)
plt.xlabel('FranchiseCode')
plt.ylabel('Default Rate')
plt.title(f'Top 10 Safest FranchiseCodes (≥ {min_loans} loans)')
plt.ylim(0, top_safe['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show() ## cungx duocj


# RevLineCr_Binary
df_clean = df[['RevLineCr_Binary', 'MIS_Status_Binary']].dropna()

# 5. Compute Pearson correlation between RevLineCr_Binary and default flag
corr_coef = df_clean['RevLineCr_Binary'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (RevLineCr_Binary vs. Default): {corr_coef:.4f}")

# 6. Calculate default rate by RevLineCr category
grouped = (
    df_clean
    .groupby('RevLineCr_Binary')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)
grouped['category'] = grouped['RevLineCr_Binary'].map({0: 'Term Loan', 1: 'Revolving Line'})

print("\nDefault rate by RevLineCr category:")
print(grouped[['category', 'loan_count', 'default_rate']])

# 7. Plot default rate by RevLineCr category
plt.figure(figsize=(6, 4))
plt.bar(
    grouped['category'],
    grouped['default_rate'],
    color=['skyblue', 'salmon'],
    alpha=0.8
)
plt.xlabel('Loan Type')
plt.ylabel('Default Rate')
plt.title('Default Rate: Term Loan vs. Revolving Line of Credit')
plt.ylim(0, grouped['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# LowDoc Binary
df_clean = df[['LowDoc_Binary', 'MIS_Status_Binary']].dropna()

# 5. Compute Pearson correlation between LowDoc_Binary and default flag
corr_coef = df_clean['LowDoc_Binary'].corr(df_clean['MIS_Status_Binary'])
print(f"Pearson correlation (LowDoc_Binary vs. Default): {corr_coef:.4f}")

# 6. Calculate default rate by LowDoc category
grouped = (
    df_clean
    .groupby('LowDoc_Binary')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)
grouped['category'] = grouped['LowDoc_Binary'].map({0: 'Standard Documentation', 1: 'Low Documentation'})

print("\nDefault rate by LowDoc category:")
print(grouped[['category', 'loan_count', 'default_rate']])

# 7. Plot default rate by LowDoc category
plt.figure(figsize=(6, 4))
plt.bar(
    grouped['category'],
    grouped['default_rate'],
    color=['steelblue', 'tomato'],
    alpha=0.8
)
plt.xlabel('Documentation Type')
plt.ylabel('Default Rate')
plt.title('Default Rate: Standard vs. Low-Documentation Loans')
plt.ylim(0, grouped['default_rate'].max() * 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Approval FY
df['ApprovalFY'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')
df_year = df[df['ApprovalFY'].notna()].copy()

# Group by Fiscal Year and compute default rate
yearly_stats = (
    df_year
    .groupby('ApprovalFY')['MIS_Status_Binary']
    .agg(loan_count='size', default_rate='mean')
    .reset_index()
)

# Plot default rate over years
plt.figure(figsize=(12, 6))
plt.plot(yearly_stats['ApprovalFY'], yearly_stats['default_rate'], marker='o', linestyle='-')
plt.title('Default Rate by Fiscal Year of Loan Approval')
plt.xlabel('Fiscal Year')
plt.ylabel('Default Rate')
plt.grid(True)
plt.tight_layout()
plt.show()

df['ApprovalFY'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')
print(df['ApprovalFY'].describe())

print("Unique values in ApprovalFY:", df['ApprovalFY'].unique())
print("Minimum ApprovalFY:", df['ApprovalFY'].min())
print("Maximum ApprovalFY:", df['ApprovalFY'].max())

# Check initial number of rows
initial_count = df.shape[0]

# Drop rows with ApprovalFY > 2025
df['ApprovalFY'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')
df_cleaned = df[df['ApprovalFY'] <= 2025]

# Check new number of rows
final_count = df_cleaned.shape[0]

# Print summary
dropped = initial_count - final_count
print(f"Initial rows: {initial_count}")
print(f"Retained rows: {final_count}")
print(f"Dropped rows: {dropped}")

# Initial rows: 899164
# Retained rows: 898993
# Dropped rows: 171

print("Unique values in ApprovalFY:", df_cleaned['ApprovalFY'].unique())
print("Minimum ApprovalFY:", df_cleaned['ApprovalFY'].min())
print("Maximum ApprovalFY:", df_cleaned['ApprovalFY'].max())
# Minimum ApprovalFY: 1962.0
# Maximum ApprovalFY: 2014.0
df = df[df['ApprovalFY'].notna()]
df['ApprovalFY_int'] = df['ApprovalFY'].astype(int)

# Create InflationRate

inflation_dict = {
    1962: 1.30, 1963: 1.60, 1964: 1.00, 1965: 1.90, 1966: 3.50, 1967: 3.00, 1968: 4.70,
    1969: 6.20, 1970: 5.60, 1971: 3.30, 1972: 3.40, 1973: 8.70, 1974: 12.30, 1975: 6.90,
    1976: 4.90, 1977: 6.70, 1978: 9.00, 1979: 13.30, 1980: 12.50, 1981: 8.90, 1982: 3.80,
    1983: 3.80, 1984: 3.90, 1985: 3.80, 1986: 1.10, 1987: 4.40, 1988: 4.40, 1989: 4.60,
    1990: 6.10, 1991: 3.10, 1992: 2.90, 1993: 2.70, 1994: 2.70, 1995: 2.50, 1996: 3.30,
    1997: 1.70, 1998: 1.60, 1999: 2.70, 2000: 3.40, 2001: 1.60, 2002: 2.40, 2003: 1.90,
    2004: 3.30, 2005: 3.40, 2006: 2.50, 2007: 4.10, 2008: 0.10, 2009: 2.70, 2010: 1.50,
    2011: 3.00, 2012: 1.70, 2013: 1.50, 2014: 0.80
}



df_cleaned['InflationRate'] = df_cleaned['ApprovalFY'].map(inflation_dict)

print(df_cleaned[['ApprovalFY', 'InflationRate']].dropna().head())

# GDP Growth
gdp_growth = {
    1962: 6.10,
    1963: 4.40,
    1964: 5.80,
    1965: 6.40,
    1966: 6.50,
    1967: 2.50,
    1968: 4.80,
    1969: 3.10,
    1970: 0.22,
    1971: 3.29,
    1972: 5.26,
    1973: 5.65,
    1974: -0.54,
    1975: -0.21,
    1976: 5.39,
    1977: 4.62,
    1978: 5.54,
    1979: 3.17,
    1980: -0.26,
    1981: 2.54,
    1982: -1.80,
    1983: 4.58,
    1984: 7.24,
    1985: 4.17,
    1986: 3.46,
    1987: 3.45,
    1988: 4.18,
    1989: 3.67,
    1990: 1.89,
    1991: -0.11,
    1992: 3.52,
    1993: 2.75,
    1994: 4.03,
    1995: 2.68,
    1996: 3.77,
    1997: 4.45,
    1998: 4.48,
    1999: 4.79,
    2000: 4.08,
    2001: 0.96,
    2002: 1.70,
    2003: 2.80,
    2004: 3.85,
    2005: 3.48,
    2006: 2.78,
    2007: 2.00,
    2008: 0.11,
    2009: -2.58,
    2010: 2.70,
    2011: 1.56,
    2012: 2.29,
    2013: 2.12,
    2014: 2.52,
}

df_cleaned['GDPGrowth'] = df_cleaned['ApprovalFY'].astype(int).map(gdp_growth)


# Interest rate
interest_rate = {
    1961: 3.11,
    1962: 3.22,
    1963: 3.37,
    1964: 2.95,
    1965: 2.57,
    1966: 2.65,
    1967: 2.41,
    1968: 1.86,
    1969: 2.85,
    1970: 2.51,
    1971: 0.62,
    1972: 0.88,
    1973: 2.41,
    1974: 1.65,
    1975: -1.28,
    1976: 1.27,
    1977: 0.58,
    1978: 1.89,
    1979: 4.03,
    1980: 5.72,
    1981: 8.59,
    1982: 8.18,
    1983: 6.62,
    1984: 8.14,
    1985: 6.56,
    1986: 6.19,
    1987: 5.59,
    1988: 5.59,
    1989: 6.69,
    1990: 6.04,
    1991: 4.92,
    1992: 3.88,
    1993: 3.55,
    1994: 4.90,
    1995: 6.59,
    1996: 6.32,
    1997: 6.60,
    1998: 7.15,
    1999: 6.49,
    2000: 6.81,
    2001: 4.57,
    2002: 3.07,
    2003: 2.11,
    2004: 1.61,
    2005: 2.96,
    2006: 4.73,
    2007: 5.20,
    2008: 3.10,
    2009: 2.62,
    2010: 2.01,
    2011: 1.16,
    2012: 1.36,
    2013: 1.52,
    2014: 1.48,
}

df_cleaned['InterestRate'] = df_cleaned['ApprovalFY'].astype(int).map(interest_rate)

# Define the final feature set (drop everything else)
features = [
    'Term',
    'DisbursementGross',
    'GuaranteeRatio',
    'FranchiseCode',
    'RevLineCr_Binary',
    'LowDoc_Binary',
    'UrbanRural_Numeric',
    'RealEstate',
    'Recession',
    'NAICS2',
    'State',
    'ApprovalFY', 
    'InterestRate',
    'GDPGrowth',
    'InflationRate']

target = 'MIS_Status_Binary'

# Create modeling DataFrame
df_model = df_cleaned[features + [target]].copy()

# Drop rows with NA in features or target
df_model.replace([np.inf, -np.inf], np.nan, inplace=True)
df_model.dropna(inplace=True)

# Identify categorical columns (to be one-hot encoded)
categorical_cols = [
    'FranchiseCode',
    'RevLineCr_Binary',
    'LowDoc_Binary',
    'UrbanRural_Numeric',
    'RealEstate',
    'Recession',
    'NAICS2',
    'State',
    'ApprovalFY'
]

df_model[categorical_cols] = df_model[categorical_cols].astype(str)

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df_model, columns=categorical_cols, drop_first=True)

# Split into features (X) and target (y)
X = df_encoded.drop(target, axis=1)
y = df_encoded[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.9,
    random_state=1)

print("X_train.shape:", X_train.shape)  
print("X_test.shape: ", X_test.shape)  

numeric_cols = [
    'Term',
    'DisbursementGross',
    'GuaranteeRatio',
    'InflationRate',
    'InterestRate',
    'GDPGrowth']

all_cols = X_train.columns.tolist()
categorical_dummy_cols = [c for c in all_cols if c not in numeric_cols]

from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('passthrough', 'passthrough', categorical_dummy_cols)],
    remainder='drop')

# -------------------------------
# NEURAL NETWORK
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve
)

# ==============================================
# 0) ASSUME YOU HAVE:
#    - df_model: DataFrame containing feature_cols, 'MIS_Status_Binary', and 'DisbursementGross'
#    - feature_cols: list of predictors
#    - numeric_cols: list of numeric predictors
#    - categorical_cols: list of categorical predictors
# ==============================================

# 1) Prepare X, y, disbursement arrays
X    = df_model[features].copy()
y    = df_model['MIS_Status_Binary'].astype(int).copy()
disb = df_model['DisbursementGross'].values.copy()

# 2) Train/Validation split (70% train, 30% val) with stratification
X_train, X_test, y_train, y_test, disb_train, disb_test = train_test_split(
    X, y, disb,
    test_size=0.90,
    random_state=1,
    stratify=y
)

# 3) Build preprocessing pipeline (same as for LR)
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numeric_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_cols)
])

mlp_pipe = Pipeline([
    ('preproc', preprocessor),
    ('mlp', MLPClassifier(random_state=42, early_stopping=True, max_iter=200))
])

# 5) Parameter lists instead of scipy.stats
param_dist = {
    'mlp__hidden_layer_sizes': [(64,32), (128,64,32), (100,)],
    'mlp__activation':         ['relu', 'tanh'],
    # pick 10 values log‐uniformly spaced between 1e‐4 and 1e‐2:
    'mlp__alpha':              list(np.logspace(-4, -2, num=10)),
    'mlp__learning_rate_init': list(np.logspace(-4, -2, num=10))
}

# 6) Randomized search
search = RandomizedSearchCV(
    estimator=mlp_pipe,
    param_distributions=param_dist,
    n_iter=20,
    scoring='roc_auc',
    cv=5,
    random_state=42,
    verbose=2,
    n_jobs=-1,
    refit=True
)
search.fit(X_train, y_train)
best_mlp = search.best_estimator_

print("\n=== Best MLP Hyperparameters ===")
print(search.best_params_)
print("Best CV ROC AUC:", search.best_score_)



# 7) Evaluate at 0.5
probs_def = best_mlp.predict_proba(X_test)[:,1]
y_pred_05 = (probs_def >= 0.5).astype(int)

print("\n--- Classification Report (threshold=0.5) ---")
print(classification_report(y_test, y_pred_05, digits=4))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_05))
roc_auc = roc_auc_score(y_test, probs_def)
print(f"Test ROC AUC: {roc_auc:.4f}")

# 8) Net profit & ROI functions
def net_profit(y_true, y_pred, disb_amt):
    return np.where(
        y_pred==0,
        np.where(y_true==0, 0.05*disb_amt, -0.25*disb_amt),
        0.0
    ).sum()

def roi(net_prof, y_pred, disb_amt):
    granted = disb_amt[y_pred==0].sum()
    return net_prof / granted if granted>0 else np.nan

profit_05 = net_profit(y_test.values, y_pred_05, disb_test)
roi_05    = roi(profit_05, y_pred_05, disb_test)
print(f"\nNet Profit at t=0.5: ${profit_05:,.2f}")
print(f"Rate of Return at t=0.5: {roi_05:.2%}")

# 9) Tune threshold
thresholds = np.arange(0,1,0.01)
profits    = []
for t in thresholds:
    y_pred_t = (probs_def >= t).astype(int)
    profits.append(net_profit(y_test.values, y_pred_t, disb_test))
profits = np.array(profits)

best_idx = np.argmax(profits)
best_t   = thresholds[best_idx]
best_p   = profits[best_idx]

print(f"\nOptimal Threshold: {best_t:.2f}")
print(f"Max Net Profit:    ${best_p:,.2f}")

# 10) Evaluate at optimal
y_pred_opt = (probs_def >= best_t).astype(int)
print("\n--- Classification Report (optimal threshold) ---")
print(classification_report(y_test, y_pred_opt, digits=4))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_opt))

profit_opt = net_profit(y_test.values, y_pred_opt, disb_test)
roi_opt    = roi(profit_opt, y_pred_opt, disb_test)
print(f"\nNet Profit at t={best_t:.2f}: ${profit_opt:,.2f}")
print(f"Rate of Return at t={best_t:.2f}: {roi_opt:.2%}")


fpr, tpr, thresholds = roc_curve(y_test, probs_def)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})', color='darkorange', lw=2)
plt.plot([0, 1], [0, 1], linestyle='--', color='navy', label='Random Classifier')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib

# Save to file
joblib.dump(best_mlp, 'best_mlp_model.joblib')
print("Model saved to best_mlp_model.joblib")


# 13) Requirement 3: Gains & Lift charts using net profit

# 1) Get predicted probabilities of “Paid-in-Full”
probs_default = best_mlp.predict_proba(X_test)[:, 1]   # P(Default)
probs_pif     = 1.0 - probs_default                    # P(PIF)

# 2) Build validation DataFrame sorted by least‐risky first
df_val = pd.DataFrame({
    'P_PIF': probs_pif,
    'y_true': y_test.values,  # 0 = PIF, 1 = Default
    'disb': disb_test
})
df_val.sort_values('P_PIF', ascending=False, inplace=True)
df_val.reset_index(drop=True, inplace=True)

# 3) Compute per‐loan net profit if granted
df_val['profit_if_granted'] = np.where(
    df_val['y_true'] == 0,
    0.05 * df_val['disb'],    # grant & PIF → +5% of disb
    -0.25 * df_val['disb']    # grant & Default → -25% of disb
)

# 4) Cumulative profit and % granted
df_val['cum_profit'] = df_val['profit_if_granted'].cumsum()
n = len(df_val)
df_val['pct_granted'] = (np.arange(n) + 1) / n

# 5a) Find how far to go for max net profit
best_idx    = df_val['cum_profit'].idxmax()
best_profit = df_val.loc[best_idx, 'cum_profit']
best_pct     = df_val.loc[best_idx, 'pct_granted'] * 100
best_count   = best_idx + 1

print(f"(a) To achieve MAX NET PROFIT = ${best_profit:,.2f},")
print(f"    grant loans to {best_pct:.2f}% of the validation set")
print(f"    (i.e. {best_count} out of {n} applications)")

# 5b) Determine the P(PIF) cut-off for future applicants
cutoff_pif = df_val.loc[best_idx, 'P_PIF']
print(f"(b) For future applicants, grant the loan if P(PIF) ≥ {cutoff_pif:.4f}")

# 6) Plot Gains Chart (Cumulative Profit vs % Granted)
plt.figure(figsize=(8,5))
plt.plot(df_val['pct_granted']*100, df_val['cum_profit'], label='Cumulative Profit', lw=2)
plt.axvline(best_pct, color='red', linestyle='--', label=f'Max @ {best_pct:.2f}%')
plt.xlabel('% of Applicants Granted')
plt.ylabel('Cumulative Net Profit (USD)')
plt.title('Gains Chart: Net Profit vs % Granted')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 7) Plot Lift Chart vs Random Baseline
mean_profit = df_val['profit_if_granted'].mean()
df_val['random_profit'] = df_val['pct_granted'] * (mean_profit * n)

plt.figure(figsize=(8,5))
plt.plot(df_val['pct_granted']*100, df_val['cum_profit'], lw=2, label='Model')
plt.plot(df_val['pct_granted']*100, df_val['random_profit'], lw=2, linestyle='--', label='Random')
plt.axvline(best_pct, color='red', linestyle='--')
plt.xlabel('% of Applicants Granted')
plt.ylabel('Cumulative Net Profit (USD)')
plt.title('Lift Chart: Net Profit vs % Granted')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()









































